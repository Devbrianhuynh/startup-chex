{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and exploring data\n",
    "startup_data = []\n",
    "\n",
    "with open('big_startup_secsees_dataset.csv', 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        startup_data.append(row)\n",
    "\n",
    "startup_df = pd.DataFrame(startup_data).drop(columns=['permalink', 'homepage_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = startup_df['status']\n",
    "# Create feature cols and drop all redundant cols (ex:, 'city' is technically the same as 'region')\n",
    "X = startup_df.drop(columns=['status', 'name', 'region', 'country_code', 'state_code']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "y = np.where((y == 'acquired') | (y == 'ipo'), 1, \n",
    "                np.where(y == 'closed', 0, np.where(y == 'operating', -1, np.nan)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns.tolist():\n",
    "    X[col] = np.where((X[col] == '-') | (X[col] == ''), np.nan, X[col])\n",
    "   \n",
    "    try:\n",
    "        X[col] = pd.to_numeric(X[col], errors='raise')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "X.info(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['founded_at', 'first_funding_at', 'last_funding_at']:\n",
    "    X[col] = X[col].apply(lambda x: x if isinstance(x, str) is False else x.split('-')[0])\n",
    "\n",
    "    X[col] = np.where(X[col] == '2105', '2015', X[col]) # One of the values in the three cols is a typo of 2105 instead of 2015\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['category_list'] = X['category_list'].apply(lambda x: x if isinstance(x, str) is False else x.split('|')[0])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X['category_list'].value_counts())\n",
    "print()\n",
    "print(X['city'].value_counts())\n",
    "print()\n",
    "print(X['funding_rounds'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())])\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('ohe', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num_pipe', num_pipe, numerical_cols), ('cat_pipe', cat_pipe, categorical_cols)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = Pipeline([('preprocessor', preprocessor), ('clf', LogisticRegression(max_iter=1000, solver='liblinear'))])\n",
    "pipe_dt = Pipeline([('preprocessor', preprocessor), ('clf', DecisionTreeClassifier(max_depth=7, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two choices: \n",
    "    # (1) Use original pipeline to train the model directly  (using the full dataset will take 1-5 secs. to train)\n",
    "    # (2) Use GridSearchCV() for better params, but with a smaller dataset (using the full dataset will take 2 hrs. to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice 1\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = pipe_lr.predict(X_test)\n",
    "y_pred_dt = pipe_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe_lr.score(X_test, y_test))\n",
    "print(pipe_dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_lr = confusion_matrix(y_test, y_pred_lr, labels=[1, 0, -1])\n",
    "index = ['actual_success', 'actual_unsuccess', 'actual_operating']\n",
    "columns = ['predicted_success', 'predicted_unsuccess', 'predicted_operating']\n",
    "\n",
    "test_confusion_matrix = pd.DataFrame(data=confusion_matrix_lr, index=index, columns=columns)\n",
    "test_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dt = confusion_matrix(y_test, y_pred_dt, labels=[1, 0, -1])\n",
    "index = ['actual_success', 'actual_unsuccess', 'actual_operating']\n",
    "columns = ['predicted_success', 'predicted_unsuccess', 'predicted_operating']\n",
    "\n",
    "test_confusion_matrix = pd.DataFrame(data=confusion_matrix_dt, index=index, columns=columns)\n",
    "test_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "startup_info = {\n",
    "    'category_list': ['Apps', 'Curated Web', 'Application Platforms', 'Cloud Computing', 'Software'],\n",
    "    'funding_total_usd': [90000000, 100000, 5000000, 12000000, 70000000],\n",
    "    'city': ['San Francisco', 'San Francisco', 'Austin', 'Palo Alto', 'Palo Alto'],\n",
    "    'funding_rounds': [2, 1, 1, 2, 3],\n",
    "    'founded_at': [2020, 2015, 2019, 2024, 2015],\n",
    "    'first_funding_at': [2021, 2018, 2020, 2024, 2021],\n",
    "    'last_funding_at': [2023, 2022, 2024, 2024, 2024]\n",
    "}\n",
    "\n",
    "startup_info = pd.DataFrame(startup_info)\n",
    "\n",
    "y_pred_lr_result = pipe_lr.predict(startup_info)\n",
    "y_pred_dt_result = pipe_dt.predict(startup_info)\n",
    "\n",
    "print(y_pred_lr_result.tolist())\n",
    "print(y_pred_dt_result.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pipe_lr.named_steps['clf'].coef_[0]\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "coef_features = list(zip(feature_names, coefficients))\n",
    "\n",
    "coef_features.sort(key=lambda x: np.abs(x[-1]), reverse=True)\n",
    "\n",
    "most_important_factors = coef_features[:3]\n",
    "most_important_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pipe_dt.named_steps['clf'].feature_importances_\n",
    "feature_names = X_train.columns.tolist()\n",
    "importances_features = list(zip(feature_names, importances))\n",
    "\n",
    "importances_features.sort(key=lambda x: np.abs(x[-1]), reverse=True)\n",
    "\n",
    "most_important_factors = importances_features[:3]\n",
    "most_important_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice 2\n",
    "param_grid = {\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': [1, 5, 10, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr = GridSearchCV(estimator=pipe_lr, param_grid=param_grid, n_jobs=5, cv=3)\n",
    "gs_dt = GridSearchCV(estimator=pipe_dt, param_grid=param_grid, n_jobs=5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset = X_train[:3000] # If the entire 40,000+ rows are used, training would take 2+ hrs\n",
    "y_train_subset = y_train[:3000]\n",
    "\n",
    "gs_lr.fit(X_train_subset, y_train_subset)\n",
    "gs_dt.fit(X_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = gs_lr.predict(X_test)\n",
    "y_pred_dt = gs_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr = accuracy_score(y_test, y_pred_lr)\n",
    "score_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(score_lr)\n",
    "print(score_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "startup_info = {\n",
    "    'category_list': ['Apps', 'Curated Web', 'Application Platforms', 'Cloud Computing', 'Software'],\n",
    "    'funding_total_usd': [90000000, 100000, 5000000, 12000000, 70000000],\n",
    "    'city': ['San Francisco', 'San Francisco', 'Austin', 'Palo Alto', 'Palo Alto'],\n",
    "    'funding_rounds': [2, 1, 1, 2, 3],\n",
    "    'founded_at': [2020, 2015, 2019, 2024, 2015],\n",
    "    'first_funding_at': [2021, 2018, 2020, 2024, 2021],\n",
    "    'last_funding_at': [2023, 2022, 2024, 2024, 2024]\n",
    "}\n",
    "\n",
    "startup_info = pd.DataFrame(startup_info)\n",
    "\n",
    "y_pred_lr_result = gs_lr.predict(startup_info)\n",
    "y_pred_dt_result = gs_dt.predict(startup_info)\n",
    "\n",
    "print(y_pred_lr_result.tolist())\n",
    "print(y_pred_dt_result.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
